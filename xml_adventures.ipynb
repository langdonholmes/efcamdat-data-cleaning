{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c87420",
   "metadata": {},
   "source": [
    "# Data Wrangling Exercise\n",
    "Purpose: Classify learners by CEFR\n",
    "\n",
    "Phase 1: Wrangle some datums\n",
    "\n",
    "Notes from meeting with Scott:\n",
    "1. Consider text length\n",
    "2. Consider how representative each text is (e.g. of a given CEFR band). I am not sure if he was alluding to outliers or something else here.\n",
    "3. Methods/Technologies to consider:\n",
    "\n",
    "    a. Semantic spaces\n",
    "\n",
    "    b. LSA (this was a strong suggestion)\n",
    "\n",
    "    c. Word2Vec\n",
    "\n",
    "## Information on EFCAMDAT\n",
    "\n",
    "> EFCAMDAT consists of essays submitted to Englishtown, the online school of EF Education First, by language learners all over the world (Education First, 2012).  A full course in Englishtown spans 16 proficiency levels aligned with common standards such as TOEFL, IELTS and the Common European Framework of Reference for languages.\n",
    "\n",
    "__[Overview of EFCAMDAT Data (2013)](https://corpus.mml.cam.ac.uk/faq/SLRF2013Geertzenetal.pdf)__\n",
    "\n",
    "__[Study with recommendations for Dependency Parsing on this data set (2018)](https://corpus.mml.cam.ac.uk/faq/IJCL2018Huangetal.pdf)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67c1a649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: /home/jovyan/work/efcamdat/efcamdat-data-cleaning\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re # one-way encryption for your codebase\n",
    "import os.path\n",
    "import unicodedata # manipulate strange unicode characters\n",
    "from IPython.display import display # Show me what's going on.\n",
    "import pandas as pd\n",
    "\n",
    "print('Working Directory set to:', os.getcwd())\n",
    "\n",
    "test_file = os.path.join(os.pardir, 'Original Files', 'Level 6 EF_camdat.txt')\n",
    "with open(test_file, \"r\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d30ad6",
   "metadata": {},
   "source": [
    "### One option is to manually alter each illegal character into well-formatted XML. The following script could help with that, but a full-featured text editor like Notepad++ might be a better fit for the job.\n",
    "__[Predefined characters in XML](https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references#Predefined_entities_in_XML)__ are mainly &, \", ', <, and >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b056baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illegal_character = '&'\n",
    "# p = re.compile(r'<text>\\n+(.+)(' + illegal_character + ')(.+)\\s+<\\/text>')\n",
    "# matches = p.findall(data)\n",
    "\n",
    "# for match in matches[:2]:\n",
    "#     print(match[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c2ad5",
   "metadata": {},
   "source": [
    "### Another option is to wrap every \\<TEXT\\> block in \\[\\[CDATA\\]\\] tags, which might magically make the XML properly formatted\n",
    "__[CDATA Sections in XML](https://www.tutorialspoint.com/xml/xml_cdata_sections.htm)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07139e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_cdata(text_masquerading_as_xml):\n",
    "    return re.sub(r'\\s*</text>',r']]></text>', re.sub(r'<text>\\s*',r'<text><![CDATA[', text_masquerading_as_xml))\n",
    "\n",
    "# cdata_blocked_off = wrap_cdata(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983246b",
   "metadata": {},
   "source": [
    "### This seems to work for some of the levels, but not all of them. Let's try to troubleshoot some more.\n",
    "Okay. It seems all(?) of the remaining issues are control characters. Python package unicodedata can handle this (thank you\n",
    "StackOverflow). One way to double check that this is working as expected would be to look at a git difference between\n",
    "cdata_blocked_off and controls_removed. This way we can ensure that only control characters are being removed.\n",
    "At first I thought Scott might have intentionally made this data difficult to work with, but this is way\n",
    "too crazy for anyone to have done it on purpose. One of the responses is literally just a string of control characters??\n",
    "\n",
    "The remove_control_characters function could be narrowed in scope to just the text within CDATA tags, but I am pretty sure there are no problem bytes anywhere else... therefore it would have the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403b5b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_control_characters(not_really_xml):\n",
    "    return \"\".join(ch for ch in not_really_xml if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "# controls_removed = remove_control_characters(cdata_blocked_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47159fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 'samples' in this level 24,789:\n"
     ]
    }
   ],
   "source": [
    "def get_writings(cleanish_xml):\n",
    "    return etree.fromstring(bytes(cleanish_xml, encoding='utf8'))[1]\n",
    "\n",
    "# parsed_xml_writings = get_writings(controls_removed)\n",
    "# print(\"The number of \\'samples\\' in this level {:,}:\".format(len(parsed_xml_writings)))\n",
    "#some of these samples are obviously useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a529186",
   "metadata": {},
   "source": [
    "### This is looking good! Now we need to build a dataframe and extract the texts.\n",
    "So we have writing IDs and Learner IDs, so do not make Learner ID the index (indexes should not have duplicate entries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1043d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just hard code this...\n",
    "\n",
    "def xml_framer(xml_root, cols):\n",
    "    lol = [] # list of lists\n",
    "    for sample in xml_root:\n",
    "        l = []\n",
    "        l.append(sample.attrib['id'])\n",
    "        l.append(sample.attrib['level'])\n",
    "        l.append(sample.attrib['unit'])\n",
    "        l.append(sample[0].attrib['id'])\n",
    "        l.append(sample[0].attrib['nationality'])\n",
    "        l.append(sample[1].text)\n",
    "        l.append(sample[1].attrib['id'])\n",
    "        l.append(sample[2].text)\n",
    "        l.append(sample[3].text)\n",
    "        l.append(sample[4].text)\n",
    "        lol.append(l)\n",
    "    df = pd.DataFrame(lol, columns=cols)\n",
    "    return df\n",
    "\n",
    "col_labels = ['id','lvl','unit','author_id','author_nationality','topic','topic_id','date','grade','text']\n",
    "\n",
    "# test_df = xml_framer(parsed_xml_writings, col_labels)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290f28ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Original Files/Level 1 EF_camdat.txt\n",
      "../Original Files/Level 2 EF_camdat.txt\n",
      "../Original Files/Level 3 EF_camdat.txt\n",
      "../Original Files/Level 4 EF_camdat.txt\n",
      "../Original Files/Level 5 EF_camdat.txt\n",
      "../Original Files/Level 6 EF_camdat.txt\n",
      "../Original Files/Level 7 EF_camdat.txt\n",
      "../Original Files/Level 8 EF_camdat.txt\n",
      "../Original Files/Level 9 EF_camdat.txt\n",
      "../Original Files/Level 10 EF_camdat.txt\n",
      "../Original Files/Level 11 EF_camdat.txt\n",
      "../Original Files/Level 12 EF_camdat.txt\n",
      "../Original Files/Level 13 EF_camdat.txt\n",
      "../Original Files/Level 14 EF_camdat.txt\n",
      "../Original Files/Level 15 EF_camdat.txt\n",
      "../Original Files/Level 16 EF_camdat.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,17):\n",
    "    file = os.path.join(os.pardir, 'Original Files', ' '.join(['Level', str(i), 'EF_camdat.txt']))\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341d594",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "If you have the RAM, might as well...\n",
    "\n",
    "**Manually fix the mismatched tags (\\<user>Theodora Alexopoulou\\</url>) in line 8 of Level 6 EF_camdat.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9eb657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lvl</th>\n",
       "      <th>unit</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_nationality</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>date</th>\n",
       "      <th>grade</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C18217</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20967052</td>\n",
       "      <td>cn</td>\n",
       "      <td>Writing labels for a clothing store</td>\n",
       "      <td>22440</td>\n",
       "      <td>2012-04-20 21:12:14.890</td>\n",
       "      <td>79</td>\n",
       "      <td>Date:monday 11th.   Time:9.30 am.  From:Margar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21016113</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-24 03:50:49.100</td>\n",
       "      <td>85</td>\n",
       "      <td>Dear teacher, My name's Yi Zhao,English is poo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C18648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20967075</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2012-04-20 08:53:55.087</td>\n",
       "      <td>90</td>\n",
       "      <td>My name's Henry Hong,  I was born in Nanjing o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C20184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-14 08:54:48.380</td>\n",
       "      <td>95</td>\n",
       "      <td>Dear Sir, I'm Jianwen Zhang, from a little tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C20185</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Taking inventory in the office</td>\n",
       "      <td>9820</td>\n",
       "      <td>2011-12-26 09:49:48.140</td>\n",
       "      <td>98</td>\n",
       "      <td>Dear Ms Thomas, There are thirteen computers a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>U718945</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>21649452</td>\n",
       "      <td>us</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-04 02:47:23.180</td>\n",
       "      <td>65</td>\n",
       "      <td>THE SPACE NEEDLE The United States's Northwest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>U719493</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24750663</td>\n",
       "      <td>ae</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-05 20:41:18.933</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a most new matter of proto and Robots ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>U722572</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>22117882</td>\n",
       "      <td>br</td>\n",
       "      <td>Using creative writing techniques</td>\n",
       "      <td>9146</td>\n",
       "      <td>2012-09-17 20:59:34.413</td>\n",
       "      <td>96</td>\n",
       "      <td>Hi Tabby,   Never mind about asking me such a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>U724086</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19328467</td>\n",
       "      <td>sa</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-23 06:55:59.733</td>\n",
       "      <td>94</td>\n",
       "      <td>Oil Tower Statue   It is a statue of transitio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>U725092</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>19643771</td>\n",
       "      <td>tw</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-27 21:40:29.393</td>\n",
       "      <td>85</td>\n",
       "      <td>The human history had thousands years and cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549281 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id lvl unit author_id author_nationality  \\\n",
       "0     C18217   1    7  20967052                 cn   \n",
       "1     C18541   1    1  21016113                 cn   \n",
       "2     C18648   1    1  20967075                 cn   \n",
       "3     C20184   1    1  18898793                 cn   \n",
       "4     C20185   1    2  18898793                 cn   \n",
       "..       ...  ..  ...       ...                ...   \n",
       "748  U718945  16    2  21649452                 us   \n",
       "749  U719493  16    1  24750663                 ae   \n",
       "750  U722572  16    5  22117882                 br   \n",
       "751  U724086  16    2  19328467                 sa   \n",
       "752  U725092  16    1  19643771                 tw   \n",
       "\n",
       "                                      topic topic_id                     date  \\\n",
       "0       Writing labels for a clothing store    22440  2012-04-20 21:12:14.890   \n",
       "1             Introducing yourself by email     3535  2011-12-24 03:50:49.100   \n",
       "2             Introducing yourself by email     3535  2012-04-20 08:53:55.087   \n",
       "3             Introducing yourself by email     3535  2011-12-14 08:54:48.380   \n",
       "4            Taking inventory in the office     9820  2011-12-26 09:49:48.140   \n",
       "..                                      ...      ...                      ...   \n",
       "748  Writing about a symbol of your country     8341  2012-09-04 02:47:23.180   \n",
       "749         Attending a robotics conference     7524  2012-09-05 20:41:18.933   \n",
       "750       Using creative writing techniques     9146  2012-09-17 20:59:34.413   \n",
       "751  Writing about a symbol of your country     8341  2012-09-23 06:55:59.733   \n",
       "752         Attending a robotics conference     7524  2012-09-27 21:40:29.393   \n",
       "\n",
       "    grade                                               text  \n",
       "0      79  Date:monday 11th.   Time:9.30 am.  From:Margar...  \n",
       "1      85  Dear teacher, My name's Yi Zhao,English is poo...  \n",
       "2      90  My name's Henry Hong,  I was born in Nanjing o...  \n",
       "3      95  Dear Sir, I'm Jianwen Zhang, from a little tow...  \n",
       "4      98  Dear Ms Thomas, There are thirteen computers a...  \n",
       "..    ...                                                ...  \n",
       "748    65  THE SPACE NEEDLE The United States's Northwest...  \n",
       "749     0  This is a most new matter of proto and Robots ...  \n",
       "750    96  Hi Tabby,   Never mind about asking me such a ...  \n",
       "751    94  Oil Tower Statue   It is a statue of transitio...  \n",
       "752    85  The human history had thousands years and cont...  \n",
       "\n",
       "[549281 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=col_labels)\n",
    "for i in range(1,17):\n",
    "    file = os.path.join(os.pardir, 'Original Files', ' '.join(['Level', str(i), 'EF_camdat.txt']))\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        df = df.append(xml_framer(get_writings(remove_control_characters(wrap_cdata(data))), col_labels))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47690072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_levels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a6d03",
   "metadata": {},
   "source": [
    "### We may want to consider running a spellchecker on these responses.\n",
    "\n",
    "Maybe we...\n",
    "\n",
    "1. Eliminate useless responses. I think we can assume that all nearly-identical responses have language taken from the prompt. Even if these responses are not just echos of the prompt, I do not think they can tell us very much about the writer, since there is very little variation between them. Like most things, I'm not sure about this.\n",
    "\n",
    "    a. I think it could be fun to calculate levenshtein distance on the responses, and eliminate responses that are too similar that way.\n",
    "    \n",
    "    b. I think the more robust method would be to calculate the TF-IDF and cosine similarity, but that seems a little complex to just find responses with low variance.\n",
    "    \n",
    "    c. We could use a cutoff with the \"Grade attribute\". Irrelevant or incomplete responses are graded lower (maybe anything below 50? below 70?).\n",
    "    \n",
    "2. Once we have a relatively useful subset of samples, we can bifurcate\n",
    "\n",
    "    a. Create .spacy on the samples\n",
    "    \n",
    "    b. Run spellchecker then create .spacy on a copy of the samples. This might not buy us anything, but it just might make a difference.\n",
    "    \n",
    "    **Actually, I think the best way to do this would be through spaCy.** I don't know if spaCy has an in-built spell checker, but we could add a spell checker to a custom spaCy pipeline. This would have the important advantage of preserving the original, misspelled response as well as a reasonable prediction of the intended orthography. I am not sure how we could incorporate both the uncorrected and corrected texts into a single model, but I sort of like the workflow here anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc61ae2",
   "metadata": {},
   "source": [
    "### Errors I've noticed\n",
    "1. Spaces before commas. I think this is **always** an error.\n",
    "2. No space after commas. I think this is an error unless the character after the comma is a quotation mark.\n",
    "3. Spaces before apostrophes/inverted commas. I think this is **always** an error.\n",
    "4. Misspellings. I suspect we can improve the data automatically by spell checking, but we of course lose information about writers' spelling knowledge.\n",
    "5. No space after periods (at end of sentence). This one is tricky. Periods should have a space when they are separating a sentence, but not when they are separating numbers (69.00 dollars) or acronyms (U.S.A.). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
