{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24129e26",
   "metadata": {},
   "source": [
    "# Data Wrangling Exercise\n",
    "Purpose: Classify learners by CEFR\n",
    "\n",
    "Phase 1: Wrangle some datums\n",
    "\n",
    "Notes from meeting with Scott:\n",
    "1. Consider text length\n",
    "2. Consider how representative each text is (e.g. of a given CEFR band). I am not sure if he was alluding to outliers or something else here.\n",
    "3. Methods/Technologies to consider:\n",
    "\n",
    "    a. Semantic spaces\n",
    "\n",
    "    b. LSA (this was a strong suggestion)\n",
    "\n",
    "    c. Word2Vec\n",
    "\n",
    "## Information on EFCAMDAT\n",
    "\n",
    "> EFCAMDAT consists of essays submitted to Englishtown, the online school of EF Education First, by language learners all over the world (Education First, 2012).  A full course in Englishtown spans 16 proficiency levels aligned with common standards such as TOEFL, IELTS and the Common European Framework of Reference for languages.\n",
    "\n",
    "__[Overview of EFCAMDAT Data (2013)](https://corpus.mml.cam.ac.uk/faq/SLRF2013Geertzenetal.pdf)__\n",
    "\n",
    "__[Study with recommendations for Dependency Parsing on this data set (2018)](https://corpus.mml.cam.ac.uk/faq/IJCL2018Huangetal.pdf)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "104fdd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: /home/jovyan/work/efcamdat/efcamdat-data-cleaning\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re # one-way encryption for your codebase\n",
    "import os.path\n",
    "import unicodedata # manipulate strange unicode characters\n",
    "from IPython.display import display # Show me what's going on.\n",
    "import pandas as pd\n",
    "\n",
    "print('Working Directory set to:', os.getcwd())\n",
    "\n",
    "# test_file = os.path.join(os.pardir, 'Original Files', 'Level 6 EF_camdat.txt')\n",
    "# with open(test_file, \"r\") as file:\n",
    "#     data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feffc60",
   "metadata": {},
   "source": [
    "### Bytes, Encodings, Unicode and Other\n",
    "\n",
    "UTF-8 seems to cover it pretty well, but there are also control characters.\n",
    "There is probably a more computationally efficient way to remove control characters, but corpus linguists like to work on character strings, so I won't worry about that too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "131852ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-899076622fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mwith_BOM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiagnosticator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_BOM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# without_BOM = diagnosticator(files, 'utf-8')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-899076622fbb>\u001b[0m in \u001b[0;36mdiagnosticator\u001b[0;34m(file_paths, codec)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#             data = re.sub(\"[^\\x20-\\x7E]\", \"\", data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^\\x20-\\x7E]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#             data = data.decode()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#             data = \"\".join(ch for ch in data if unicodedata.category(ch)[0]==\"C\" and ch != '\\n') # This seems too slow...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "files = []\n",
    "for i in range(1,17):\n",
    "    file = os.path.join(os.pardir, 'Original Files', ' '.join(['Level', str(i), 'EF_camdat.txt']))\n",
    "    files.append(file)\n",
    "  \n",
    "\n",
    "punc = string.punctuations.encode()\n",
    "\n",
    "text = b\"I love \\xf0\\x9f\\xa7\\xb8 so much!\" \n",
    "\n",
    "clean_text = text.translate(None, punc)\n",
    "\n",
    "def diagnosticator(file_paths, codec):    \n",
    "    output = []\n",
    "    for file in file_paths:\n",
    "        with open(file, \"rb\") as f:\n",
    "            data = f.read()\n",
    "#             data = re.sub(\"[^\\x20-\\x7E]\", \"\", data)\n",
    "            print(re.findall(\"[^\\x20-\\x7E]\", data)[:150])\n",
    "#             data = data.decode()\n",
    "#             data = \"\".join(ch for ch in data if unicodedata.category(ch)[0]==\"C\" and ch != '\\n') # This seems too slow...\n",
    "#             output.append(data)\n",
    "    return output\n",
    "            \n",
    "\n",
    "with_BOM = diagnosticator(files, 'utf-8-sig')\n",
    "\n",
    "# without_BOM = diagnosticator(files, 'utf-8')\n",
    "# print(without_BOM)\n",
    "# def getcorpus(corpusdir):\n",
    "#     texts = []\n",
    "#     for file in Path(corpusdir).rglob(\"*/*.txt\"):\n",
    "#         with open(file, 'r', encoding=\"utf-8\") as f: \n",
    "#             # cp1252 is a Windows codec that may work for your corpus. \n",
    "#             # utf-8-sig is another option for BOM errors.\n",
    "#             data = f.read()\n",
    "#             data = re.sub(r\"\\s+\", r\" \", data) \n",
    "#             texts.append((data, file.name))\n",
    "#     return texts\n",
    "\n",
    "# texts = getcorpus(corpusdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12506b7",
   "metadata": {},
   "source": [
    "### Convert the text to XML\n",
    "One option is to manually alter each illegal character into well-formatted XML. A full-featured text editor like Notepad++ might be a good fit for the job.\n",
    "Another option is to wrap every `<TEXT>` block in `<![CDATA[]]>` tags, which might magically make the XML properly formatted.\n",
    "\n",
    "__[Predefined characters in XML](https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references#Predefined_entities_in_XML)__ are mainly `& \"  '  <  and >`\n",
    "\n",
    "__[CDATA Sections in XML](https://www.tutorialspoint.com/xml/xml_cdata_sections.htm)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35cdb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like getting rid of excess whitespace, but let's temporarily remove that feature so we can examine the Diff more clearly.\n",
    "# def wrap_cdata(text_masquerading_as_xml):\n",
    "#     return re.sub(r'\\s*</text>',r']]></text>', re.sub(r'<text>\\s*',r'<text><![CDATA[', text_masquerading_as_xml))\n",
    "\n",
    "def wrap_cdata(text_masquerading_as_xml):\n",
    "    return re.sub(r'</text>',r']]></text>', re.sub(r'<text>',r'<text><![CDATA[', text_masquerading_as_xml))\n",
    "\n",
    "# cdata_blocked_off = wrap_cdata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "508be4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_control_characters(not_really_xml):\n",
    "    return \"\".join(ch for ch in not_really_xml if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "# test_file = os.path.join(os.pardir, 'Original Files', 'Level 3 EF_camdat.txt')\n",
    "# with open(test_file, \"r\") as file:\n",
    "#     raw_test = file.read()\n",
    "#     clean_test = remove_control_characters(wrap_cdata(raw_test))\n",
    "    \n",
    "# for c in clean_test:\n",
    "#     print(i, '%04x' % ord(c), unicodedata.category(c), end=\" \")\n",
    "    \n",
    "    \n",
    "# controls_removed = remove_control_characters(cdata_blocked_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2ab31837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_writings(cleanish_xml):\n",
    "    return etree.fromstring(bytes(cleanish_xml, encoding='utf8'))[1]\n",
    "\n",
    "# parsed_xml_writings = get_writings(controls_removed)\n",
    "# print(\"The number of \\'samples\\' in this level {:,}:\".format(len(parsed_xml_writings)))\n",
    "#some of these samples are obviously useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287a639",
   "metadata": {},
   "source": [
    "### Diff\n",
    "Ok, there are some discrepancies between Shatz's report and the data I am seeing. Let's actually look at the git diffs to see if I have made a mistake. This will be a good tool to acquire anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514667c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from pprint import pprint\n",
    "\n",
    "test_file = os.path.join(os.pardir, 'Original Files', 'Level 16 EF_camdat.txt')\n",
    "with open(test_file, \"r\") as file:\n",
    "    raw_test = file.read()\n",
    "    clean_test = remove_control_characters(wrap_cdata(raw_test))\n",
    "\n",
    "d = difflib.Differ()\n",
    "text1 = raw_test.splitlines(keepends=True)\n",
    "text2 = clean_test.splitlines(keepends=True)\n",
    "\n",
    "result = list(d.compare(text1,text2))\n",
    "for line in result[-100:]:\n",
    "    if line[0] in ['-','+','?']:\n",
    "        pprint(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0b69a",
   "metadata": {},
   "source": [
    "### This is looking good! Now we need to build a dataframe and extract the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "472725c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just hard code this...\n",
    "\n",
    "def xml_framer(xml_root, cols):\n",
    "    lol = [] # list of lists\n",
    "    for sample in xml_root:\n",
    "        l = []\n",
    "        l.append(sample.attrib['id'])\n",
    "        l.append(sample.attrib['level'])\n",
    "        l.append(sample.attrib['unit'])\n",
    "        l.append(sample[0].attrib['id'])\n",
    "        l.append(sample[0].attrib['nationality'])\n",
    "        l.append(sample[1].text)\n",
    "        l.append(sample[1].attrib['id'])\n",
    "        l.append(sample[2].text)\n",
    "        l.append(sample[3].text)\n",
    "        l.append(sample[4].text)\n",
    "        lol.append(l)\n",
    "    df = pd.DataFrame(lol, columns=cols)\n",
    "    return df\n",
    "\n",
    "col_labels = ['id','lvl','unit','author_id','author_nationality','topic','topic_id','date','grade','text']\n",
    "\n",
    "# test_df = xml_framer(parsed_xml_writings, col_labels)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94b1e1",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "If you have the RAM, might as well...\n",
    "\n",
    "**Manually fix the mismatched tags (`<user>Theodora Alexopoulou</url>`) in line 8 of Level 6 EF_camdat.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8aebf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lvl</th>\n",
       "      <th>unit</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_nationality</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>date</th>\n",
       "      <th>grade</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C18217</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20967052</td>\n",
       "      <td>cn</td>\n",
       "      <td>Writing labels for a clothing store</td>\n",
       "      <td>22440</td>\n",
       "      <td>2012-04-20 21:12:14.890</td>\n",
       "      <td>79</td>\n",
       "      <td>Date:monday 11th.   Time:9.30 am. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21016113</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-24 03:50:49.100</td>\n",
       "      <td>85</td>\n",
       "      <td>Dear teacher, My name's Yi Zhao,En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C18648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20967075</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2012-04-20 08:53:55.087</td>\n",
       "      <td>90</td>\n",
       "      <td>My name's Henry Hong,  I was born ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C20184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-14 08:54:48.380</td>\n",
       "      <td>95</td>\n",
       "      <td>Dear Sir, I'm Jianwen Zhang, from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C20185</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Taking inventory in the office</td>\n",
       "      <td>9820</td>\n",
       "      <td>2011-12-26 09:49:48.140</td>\n",
       "      <td>98</td>\n",
       "      <td>Dear Ms Thomas, There are thirteen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549276</th>\n",
       "      <td>U718945</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>21649452</td>\n",
       "      <td>us</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-04 02:47:23.180</td>\n",
       "      <td>65</td>\n",
       "      <td>THE SPACE NEEDLE The United States...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549277</th>\n",
       "      <td>U719493</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24750663</td>\n",
       "      <td>ae</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-05 20:41:18.933</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a most new matter of proto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549278</th>\n",
       "      <td>U722572</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>22117882</td>\n",
       "      <td>br</td>\n",
       "      <td>Using creative writing techniques</td>\n",
       "      <td>9146</td>\n",
       "      <td>2012-09-17 20:59:34.413</td>\n",
       "      <td>96</td>\n",
       "      <td>Hi Tabby,   Never mind about askin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549279</th>\n",
       "      <td>U724086</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19328467</td>\n",
       "      <td>sa</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-23 06:55:59.733</td>\n",
       "      <td>94</td>\n",
       "      <td>Oil Tower Statue   It is a statue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549280</th>\n",
       "      <td>U725092</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>19643771</td>\n",
       "      <td>tw</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-27 21:40:29.393</td>\n",
       "      <td>85</td>\n",
       "      <td>The human history had thousands ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549281 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id lvl unit author_id author_nationality  \\\n",
       "0        C18217   1    7  20967052                 cn   \n",
       "1        C18541   1    1  21016113                 cn   \n",
       "2        C18648   1    1  20967075                 cn   \n",
       "3        C20184   1    1  18898793                 cn   \n",
       "4        C20185   1    2  18898793                 cn   \n",
       "...         ...  ..  ...       ...                ...   \n",
       "549276  U718945  16    2  21649452                 us   \n",
       "549277  U719493  16    1  24750663                 ae   \n",
       "549278  U722572  16    5  22117882                 br   \n",
       "549279  U724086  16    2  19328467                 sa   \n",
       "549280  U725092  16    1  19643771                 tw   \n",
       "\n",
       "                                         topic topic_id  \\\n",
       "0          Writing labels for a clothing store    22440   \n",
       "1                Introducing yourself by email     3535   \n",
       "2                Introducing yourself by email     3535   \n",
       "3                Introducing yourself by email     3535   \n",
       "4               Taking inventory in the office     9820   \n",
       "...                                        ...      ...   \n",
       "549276  Writing about a symbol of your country     8341   \n",
       "549277         Attending a robotics conference     7524   \n",
       "549278       Using creative writing techniques     9146   \n",
       "549279  Writing about a symbol of your country     8341   \n",
       "549280         Attending a robotics conference     7524   \n",
       "\n",
       "                           date grade  \\\n",
       "0       2012-04-20 21:12:14.890    79   \n",
       "1       2011-12-24 03:50:49.100    85   \n",
       "2       2012-04-20 08:53:55.087    90   \n",
       "3       2011-12-14 08:54:48.380    95   \n",
       "4       2011-12-26 09:49:48.140    98   \n",
       "...                         ...   ...   \n",
       "549276  2012-09-04 02:47:23.180    65   \n",
       "549277  2012-09-05 20:41:18.933     0   \n",
       "549278  2012-09-17 20:59:34.413    96   \n",
       "549279  2012-09-23 06:55:59.733    94   \n",
       "549280  2012-09-27 21:40:29.393    85   \n",
       "\n",
       "                                                     text  \n",
       "0                   Date:monday 11th.   Time:9.30 am. ...  \n",
       "1                   Dear teacher, My name's Yi Zhao,En...  \n",
       "2                   My name's Henry Hong,  I was born ...  \n",
       "3                   Dear Sir, I'm Jianwen Zhang, from ...  \n",
       "4                   Dear Ms Thomas, There are thirteen...  \n",
       "...                                                   ...  \n",
       "549276              THE SPACE NEEDLE The United States...  \n",
       "549277              This is a most new matter of proto...  \n",
       "549278              Hi Tabby,   Never mind about askin...  \n",
       "549279              Oil Tower Statue   It is a statue ...  \n",
       "549280              The human history had thousands ye...  \n",
       "\n",
       "[549281 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=col_labels)\n",
    "for i in range(1,17):\n",
    "    file = os.path.join(os.pardir, 'Original Files', ' '.join(['Level', str(i), 'EF_camdat.txt']))\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        df = df.append(xml_framer(get_writings(remove_control_characters(wrap_cdata(data))), col_labels)).reset_index(drop=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e893c3",
   "metadata": {},
   "source": [
    "### Diagnostic Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "31b52234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549281"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "84993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3535     42058\n",
       "9820     25742\n",
       "8967     22243\n",
       "5322     21124\n",
       "8572     17395\n",
       "         ...  \n",
       "28243       47\n",
       "23235       45\n",
       "28282       39\n",
       "225         21\n",
       "28257       13\n",
       "Name: topic_id, Length: 156, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "br    191000\n",
       "cn     96843\n",
       "ru     44187\n",
       "mx     41115\n",
       "de     29192\n",
       "fr     22146\n",
       "it     20934\n",
       "sa     16858\n",
       "us     14166\n",
       "tw     13596\n",
       "jp     10672\n",
       "Name: author_nationality, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(set(df['id']))) # 551,036 reported\n",
    "display(len(set(df['author_id']))) # 84,846 reported\n",
    "display(len(set(df['author_nationality']))) # 172 reported\n",
    "\n",
    "display(df['topic_id'].value_counts()[df['topic_id'].value_counts() > 10])\n",
    "display(df['author_nationality'].value_counts().head(11))\n",
    "# display(df[df['text'].str.contains('</br>', regex=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de8b1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_levels.csv') # about 200MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3178fe2",
   "metadata": {},
   "source": [
    "### We may want to consider running a spellchecker on these responses.\n",
    "\n",
    "Maybe we...\n",
    "\n",
    "1. Eliminate useless responses. I think we can assume that all nearly-identical responses have language taken from the prompt. Even if these responses are not just echos of the prompt, I do not think they can tell us very much about the writer, since there is very little variation between them. Like most things, I'm not sure about this.\n",
    "\n",
    "    a. I think it could be fun to calculate levenshtein distance on the responses, and eliminate responses that are too similar that way.\n",
    "    \n",
    "    b. I think the more robust method would be to calculate the TF-IDF and cosine similarity, but that seems a little complex to just find responses with low variance.\n",
    "    \n",
    "    c. We could use a cutoff with the \"Grade attribute\". Irrelevant or incomplete responses are graded lower (maybe anything below 50? below 70?).\n",
    "    \n",
    "2. Once we have a relatively useful subset of samples, we can bifurcate\n",
    "\n",
    "    a. Create .spacy on the samples\n",
    "    \n",
    "    b. Run spellchecker then create .spacy on a copy of the samples. This might not buy us anything, but it just might make a difference.\n",
    "    \n",
    "    **Actually, I think the best way to do this would be through spaCy.** I don't know if spaCy has an in-built spell checker, but we could add a spell checker to a custom spaCy pipeline. This would have the important advantage of preserving the original, misspelled response as well as a reasonable prediction of the intended orthography. I am not sure how we could incorporate both the uncorrected and corrected texts into a single model, but I sort of like the workflow here anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d98ead",
   "metadata": {},
   "source": [
    "### Errors I've noticed\n",
    "1. Spaces before commas. I think this is **always** an error.\n",
    "2. No space after commas. I think this is an error unless the character after the comma is a quotation mark.\n",
    "3. Spaces before apostrophes/inverted commas. I think this is **always** an error.\n",
    "4. Misspellings. I suspect we can improve the data automatically by spell checking, but we of course lose information about writers' spelling knowledge.\n",
    "5. No space after periods (at end of sentence). This one is tricky. Periods should have a space when they are separating a sentence, but not when they are separating numbers (69.00 dollars) or acronyms (U.S.A.). \n",
    "\n",
    "spaCy handles most of these errors pretty well I think. It's probably not worth worrying about them. Even where I have marked the error as highly predictable (**always**), a lot of testing would be necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
