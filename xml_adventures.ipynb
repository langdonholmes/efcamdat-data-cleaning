{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3e33d7",
   "metadata": {},
   "source": [
    "# Data Wrangling Exercise\n",
    "Purpose: Classify learners by CEFR\n",
    "\n",
    "Phase 1: Wrangle some datums\n",
    "\n",
    "Notes from meeting with Scott:\n",
    "1. Consider text length\n",
    "2. Consider how representative each text is (e.g. of a given CEFR band). I am not sure if he was alluding to outliers or something else here.\n",
    "3. Methods/Technologies to consider:\n",
    "\n",
    "    a. Semantic spaces\n",
    "\n",
    "    b. LSA (this was a strong suggestion)\n",
    "\n",
    "    c. Word2Vec\n",
    "\n",
    "## Information on EFCAMDAT\n",
    "\n",
    "> EFCAMDAT consists of essays submitted to Englishtown, the online school of EF Education First, by language learners all over the world (Education First, 2012).  A full course in Englishtown spans 16 proficiency levels aligned with common standards such as TOEFL, IELTS and the Common European Framework of Reference for languages.\n",
    "\n",
    "__[Overview of EFCAMDAT Data (2013)](https://corpus.mml.cam.ac.uk/faq/SLRF2013Geertzenetal.pdf)__\n",
    "\n",
    "__[Study with recommendations for Dependency Parsing on this data set (2018)](https://corpus.mml.cam.ac.uk/faq/IJCL2018Huangetal.pdf)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c0739f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory set to: /home/jovyan/work/efcamdat/efcamdat-data-cleaning\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re # one-way encryption for your codebase\n",
    "import os.path\n",
    "import unicodedata # manipulate strange unicode characters\n",
    "from IPython.display import display # Show me what's going on.\n",
    "import pandas as pd\n",
    "\n",
    "print('Working Directory set to:', os.getcwd())\n",
    "\n",
    "# test_file = os.path.join(os.pardir, 'Original Files', 'Level 6 EF_camdat.txt')\n",
    "# with open(test_file, \"r\") as file:\n",
    "#     data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3900ca0",
   "metadata": {},
   "source": [
    "### Convert the text to XML\n",
    "One option is to manually alter each illegal character into well-formatted XML. A full-featured text editor like Notepad++ might be a good fit for the job.\n",
    "Another option is to wrap every `<TEXT>` block in `<![CDATA[]]>` tags, which might magically make the XML properly formatted.\n",
    "\n",
    "__[Predefined characters in XML](https://en.wikipedia.org/wiki/List_of_XML_and_HTML_character_entity_references#Predefined_entities_in_XML)__ are mainly `& \"  '  <  and >`\n",
    "\n",
    "__[CDATA Sections in XML](https://www.tutorialspoint.com/xml/xml_cdata_sections.htm)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "88e7a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like getting rid of excess whitespace, but let's temporarily remove that feature so we can examine the Diff more clearly.\n",
    "# def wrap_cdata(text_masquerading_as_xml):\n",
    "#     return re.sub(r'\\s*</text>',r']]></text>', re.sub(r'<text>\\s*',r'<text><![CDATA[', text_masquerading_as_xml))\n",
    "\n",
    "def wrap_cdata(text_masquerading_as_xml):\n",
    "    return re.sub(r'</text>',r']]></text>', re.sub(r'<text>',r'<text><![CDATA[', text_masquerading_as_xml))\n",
    "\n",
    "# cdata_blocked_off = wrap_cdata(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233480",
   "metadata": {},
   "source": [
    "### This seems to work for some of the levels, but not all of them. Let's try to troubleshoot some more.\n",
    "Okay. It seems all(?) of the remaining issues are control characters. Python package unicodedata can handle this (thank you\n",
    "StackOverflow). One way to double check that this is working as expected would be to look at a git difference between\n",
    "cdata_blocked_off and controls_removed. This way we can ensure that only control characters are being removed.\n",
    "At first I thought Scott might have intentionally made this data difficult to work with, but this is way\n",
    "too crazy for anyone to have done it on purpose. One of the responses is literally just a string of control characters??\n",
    "\n",
    "The remove_control_characters function could be narrowed in scope to just the text within CDATA tags, but I am pretty sure there are no problem bytes anywhere else... therefore it would have the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "11a7e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_control_characters(not_really_xml):\n",
    "    return \"\".join(ch for ch in not_really_xml if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "# test_file = os.path.join(os.pardir, 'Original Files', 'Level 3 EF_camdat.txt')\n",
    "# with open(test_file, \"r\") as file:\n",
    "#     raw_test = file.read()\n",
    "#     clean_test = remove_control_characters(wrap_cdata(raw_test))\n",
    "    \n",
    "# for c in clean_test:\n",
    "#     print(i, '%04x' % ord(c), unicodedata.category(c), end=\" \")\n",
    "    \n",
    "    \n",
    "# controls_removed = remove_control_characters(cdata_blocked_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454da9df",
   "metadata": {},
   "source": [
    "Ok, there are some discrepancies between Shatz's report and the data I am seeing. Let's actually look at the git diffs to see if I have made a mistake. This will be a good tool to acquire anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "835874d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'-             <learner id=\"18538337\" nationality=\"br\"/>\\n'\n",
      "'-             <topic id=\"9186\">Researching a legendary creature</topic>\\n'\n",
      "'-             <date>2012-08-06 19:33:11.493</date>\\n'\n",
      "'-             <grade>89</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             Legend: Chupacabra  Description: Animal that sucks the blood '\n",
      " 'of farm animals, especially goats.  Background information: Attack and '\n",
      " 'drinks blood of farm animals.  Physical features: Size of small bear; row of '\n",
      " 'spines from neck to tail; mixture of a dog, rat and kangaroo; fangs; '\n",
      " 'hairless; reptile-like with scales; stands 3-4 feet and hops like a '\n",
      " 'kangaroo; forked tongue; hisses and screeches and eyes glow red; has a bad '\n",
      " 'stench.  First reported: in Puerto Rico in 1995. Eight shep were attacked '\n",
      " 'and blood sucked out. Only puncture marks are found on animal. Months after '\n",
      " 'his report, 150 farm animals have blood drained and small puncture wounds.  '\n",
      " 'Sightings: Puerto Rico, Mexico, Latin America, United States.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U713833\" level=\"16\" unit=\"1\">\\n'\n",
      "'-             <learner id=\"19568103\" nationality=\"fr\"/>\\n'\n",
      "'-             <topic id=\"7524\">Attending a robotics conference</topic>\\n'\n",
      "'-             <date>2012-08-15 08:48:37.900</date>\\n'\n",
      "'-             <grade>100</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             Three robot designs are well appreciated for their useful and '\n",
      " 'practical purposes. Hiro Takashi, a Japanese scientist, has worked on  '\n",
      " 'anthropomorphic robots that play an important role as  mobile escort robots. '\n",
      " 'It helps to carry out dangerous work in an effective time manner. Hiro '\n",
      " 'predicts to finish this project in 5 years with improvement of facial '\n",
      " 'expression like sadness and happiness. Emotional intelligence seems to be '\n",
      " \"the same goal for these 3 teams in making a perfect robot. Hiro's procedure \"\n",
      " \"requires a shortest time to accomplish compared to Bonnie's ( 10 years) and \"\n",
      " \"Major Pettegrew's ( by 2050). While Bonnie wants to make her robots \"\n",
      " 'serviceable in Medicine and Health, like doing hospital cleaning at the '\n",
      " \"first time and then taking care and sharing patient's suffering, Major \"\n",
      " 'Pettigrew develops industrial ones to replace stressful work in agriculture '\n",
      " 'or in security like robot dogs. As he believed, these  will become a next '\n",
      " 'generation of heros and a team of emotional human-like robots.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U714073\" level=\"16\" unit=\"8\">\\n'\n",
      "'-             <learner id=\"18910851\" nationality=\"it\"/>\\n'\n",
      "'-             <topic id=\"9186\">Researching a legendary creature</topic>\\n'\n",
      "'-             <date>2012-08-16 05:29:27.910</date>\\n'\n",
      "'-             <grade>89</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             In my country, in Campania (Italy), every 7 years, there is a '\n",
      " 'particular event, the RITI SETTENNALI, in honor of the Virgin Mary. this '\n",
      " 'event is a big mistery, because people that partecipate are unrecognizable. '\n",
      " 'They put a sponge with pins on their chest, causing the laekage of blood, '\n",
      " \"and it isn't a good scene to see. In addition, we don't know the persons \"\n",
      " 'that partecipate to this event because they wear a white hood.  There are a '\n",
      " 'lot of people that partecipate. They came from all the world .\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U715523\" level=\"16\" unit=\"8\">\\n'\n",
      "'-             <learner id=\"18507049\" nationality=\"br\"/>\\n'\n",
      "'-             <topic id=\"9186\">Researching a legendary creature</topic>\\n'\n",
      "'-             <date>2012-08-21 23:02:39.120</date>\\n'\n",
      "'-             <grade>88</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "(\"-             Hello, I've never heard in my life about 'Chupacabra'. I am \"\n",
      " 'little bit surprise with some information about him. Where I live is not '\n",
      " 'easy to hear something similar as this it. Now, I will show a summary about '\n",
      " \"'Chupacabra'. Before, have you ever seen or heard about it? If no, pay \"\n",
      " 'attention now. It an animal that sucks the blood of farm animals, especially '\n",
      " 'goats. It attacks and drinks blood of farm animals. Look, it, ‘Chupacabra’, '\n",
      " 'likes blood of others animals.  Sightings, Puerto Rico, Mexico, Latin '\n",
      " 'America, USA .Then,  its size of small bear; row of spines from neck to '\n",
      " 'tail; mixture of a dog, rat, and kangaroo; fangs; hairless; reptile-like '\n",
      " 'with scales; stands 3-4 feet and hops like a kangaroo; forked tongue; hisses '\n",
      " 'and screeches and eyes glow red; has a bad stench. First reported in Puerto '\n",
      " 'Rico in 1995. Eight sheep were attacked and blood sucked out. . Months after '\n",
      " \"this report, 150 farm animals' blood also sucked out. All animals have blood \"\n",
      " 'drained and small puncture wounds. These are some informations about '\n",
      " 'Chupacabra. Thanks, Heilan Farias.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U715734\" level=\"16\" unit=\"6\">\\n'\n",
      "'-             <learner id=\"24330011\" nationality=\"fr\"/>\\n'\n",
      "'-             <topic id=\"8177\">Applying for a home loan</topic>\\n'\n",
      "'-             <date>2012-08-22 17:54:42.357</date>\\n'\n",
      "'-             <grade>97</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             Dear John  Iam writing this email in order to request a '\n",
      " 'banking loan to buy the house Iam living in.  I have a wage of 3500 euros '\n",
      " 'per month. All my expenses cost 2000 euros per month.  Here is the detail of '\n",
      " 'my monthly expenses   1)  rent of my house cost 1000 euros    2) '\n",
      " 'electricity, telephone and other electronical expenses cost 300 euros    3) '\n",
      " 'groceries and other food expenses 400 euros    4) entertainments cost near '\n",
      " 'than 300 euros   I can save more than 1000 euros per month.my savings '\n",
      " 'account balance shows an amount of 15000 euros which can serve as first '\n",
      " 'deposit.  I have also joined a detailed and outlined budget which shows my '\n",
      " 'expenses based only on really needed things.  I have no frivolous spending '\n",
      " 'and my budget is really strict and is monthly listed and followed '\n",
      " 'rigorously.  Here are my banking statements for proof of my capacity of '\n",
      " 'honoring my debt.  I also join all documents requested:  1) driver’s '\n",
      " 'license   2) credit card   3) bank statement   4) property purchase '\n",
      " 'contract  You can join me on ert@ef.com for more details if needed   Best '\n",
      " 'regards\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U716245\" level=\"16\" unit=\"6\">\\n'\n",
      "'-             <learner id=\"18979056\" nationality=\"tw\"/>\\n'\n",
      "'-             <topic id=\"8177\">Applying for a home loan</topic>\\n'\n",
      "'-             <date>2012-08-24 17:22:29.910</date>\\n'\n",
      "'-             <grade>90</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             I want to apply for a bank loan to buy a new house. There are '\n",
      " 'some of my private informations as follows: (1) Name: MISS Rosa R BENNET (2) '\n",
      " 'Bank name of my credit card: Wirecommon Bank of       Australia. (3) '\n",
      " 'Address: 3 TEAL COURT MORNINGTON, VIC 3931 (4) Email '\n",
      " 'address:rosabennet@harroldson&sons.com.au (5) Bank name of my account: '\n",
      " 'THOMSTOWN SOUTH VIC.      Bank, State & Branch number (BSB) 76 4253. (6) The '\n",
      " 'amount of my loan:$160,000 (7) The interest rate need to  be less than 5.7% '\n",
      " 'amortized       over 20 years. (8) Written proof of a loan commitment should '\n",
      " 'be provided      on or before 15th Novermber.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U718945\" level=\"16\" unit=\"2\">\\n'\n",
      "'-             <learner id=\"21649452\" nationality=\"us\"/>\\n'\n",
      "('-             <topic id=\"8341\">Writing about a symbol of your '\n",
      " 'country</topic>\\n')\n",
      "'-             <date>2012-09-04 02:47:23.180</date>\\n'\n",
      "'-             <grade>65</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "(\"-             THE SPACE NEEDLE The United States's Northwest region has been \"\n",
      " 'mostly known in the last 50 years for The Space Needle , and it actually is '\n",
      " 'a  major landmark of Northwest  and symbolizes our beautiful city; Seattle.  '\n",
      " 'It not only symbolizes a lot for our city ,it is also our pride, the needle '\n",
      " \"has appeared in numerous films , the most famous one is ''Sleepless in \"\n",
      " \"Seattle''. It was built for the 1962 world's fair and after that it has \"\n",
      " 'become an awesome attraction for tourists from all around the world . '\n",
      " 'Visitors can reach the top of the Space needle via elevators that travel at '\n",
      " '10 miles per hour , and on windy days they are slowed down to 5 miles . At '\n",
      " 'the top there is a restaurant in one of the 2 floors ,and on the top floor '\n",
      " 'there is an observation desk from where you can see the city in 360 degrees '\n",
      " \"view. I'm so proud from the Space needle and I'm so proud of my city.\\n\")\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U719493\" level=\"16\" unit=\"1\">\\n'\n",
      "'-             <learner id=\"24750663\" nationality=\"ae\"/>\\n'\n",
      "'-             <topic id=\"7524\">Attending a robotics conference</topic>\\n'\n",
      "'-             <date>2012-09-05 20:41:18.933</date>\\n'\n",
      "'-             <grade>0</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             This is a most new matter of proto and Robots is a web-based '\n",
      " 'magazine on improvement available by Massachusetts Hospital and describe '\n",
      " 'growth of robots in charming detail. Untimely trials show that robots Tips '\n",
      " 'can facilitate children with autism. CosmoBot is a 16-inch-tall robot '\n",
      " 'trained Libby and this is a six year old with relentless autism to copy '\n",
      " 'actions in a easy Simon way and the Libby is not respond to human '\n",
      " 'instruction efforts, actually CosmoBot’s patient duplication did job. And '\n",
      " 'India I have experimental amazing example of “socially assistive” human '\n",
      " 'health care in which poor populace passed out recurring health care tasks '\n",
      " 'with old with skill and sensitivity.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U722572\" level=\"16\" unit=\"5\">\\n'\n",
      "'-             <learner id=\"22117882\" nationality=\"br\"/>\\n'\n",
      "'-             <topic id=\"9146\">Using creative writing techniques</topic>\\n'\n",
      "'-             <date>2012-09-17 20:59:34.413</date>\\n'\n",
      "'-             <grade>96</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             Hi Tabby,   Never mind about asking me such a favor. I jotted '\n",
      " 'my notes down, so I had to spend some time reorganizing it. There it goes: '\n",
      " '1) Mind Mapping: Ideal for Left-brained people. Firstly, write out your '\n",
      " \"ideas, one after another, in a linear way. Don't you be so strict, by the \"\n",
      " 'start, about grammar. The key are the ideas. 2) Listing (also known as '\n",
      " 'cluttering or diagramming). Write down your ideas randomly. At first, note '\n",
      " 'down a key word in a blank piece of paper. Then put it in a \"cube like\" '\n",
      " 'drawing, on your sheet of paper. Answer that key word with subheadings. Use '\n",
      " 'arrows or lines pointing out relationship among them. 3) Questioning. Try to '\n",
      " 'figure out the answers you might have to respond, about the topic, using '\n",
      " '\"yes\" or \"no\" questions. 4) Cubing. This  is a way of looking at a topic '\n",
      " 'from six sides. Initially, try to describe it using six statements. Spend 30 '\n",
      " 'to 50 minutes. Then look at them, and try to be as much comprehensive as you '\n",
      " 'can in your description.\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U724086\" level=\"16\" unit=\"2\">\\n'\n",
      "'-             <learner id=\"19328467\" nationality=\"sa\"/>\\n'\n",
      "('-             <topic id=\"8341\">Writing about a symbol of your '\n",
      " 'country</topic>\\n')\n",
      "'-             <date>2012-09-23 06:55:59.733</date>\\n'\n",
      "'-             <grade>94</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             Oil Tower Statue   It is a statue of transition from scarcity '\n",
      " 'to abundance from one of the poorest country to one of richest country from '\n",
      " 'illiterates to educated people.  The oil been discovered in my country in '\n",
      " '1950 by American Engineers who strived a lot till they got the black gold. '\n",
      " 'It was unbelievable event and once the king heard that announcement came '\n",
      " 'immediately and appreciated these people who worked hard to make the dream '\n",
      " 'true.  Today we are enjoying our life, we still grateful to that workers for '\n",
      " 'their achievements not only us as citizens but the six millions immigrants '\n",
      " 'who living with us . Without oil we won’t be able host these numbers of '\n",
      " 'immigrants and we won’t able to support millions of people across the '\n",
      " 'nations.  Looking ahead, by 2035 our country will be the strongest economics '\n",
      " 'in the world but not a easy task, but we will work hard to achieve it .\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-         <writing id=\"U725092\" level=\"16\" unit=\"1\">\\n'\n",
      "'-             <learner id=\"19643771\" nationality=\"tw\"/>\\n'\n",
      "'-             <topic id=\"7524\">Attending a robotics conference</topic>\\n'\n",
      "'-             <date>2012-09-27 21:40:29.393</date>\\n'\n",
      "'-             <grade>85</grade>\\n'\n",
      "'-             <text>\\n'\n",
      "('-             The human history had thousands years and continue, the '\n",
      " \"machines that we made up more than we can't count on. The most beautiful and \"\n",
      " 'refinement machines is robots, but even the robots has many types different '\n",
      " 'types has different function. Now I comparing three types of robots to you '\n",
      " 'guys, the first one is my favorite, this robot even look exactly like a '\n",
      " \"human, if I don't tell you it's a robot you will never know it. What a \"\n",
      " \"amazing work in the world, it's can do anything for you, even something \"\n",
      " \"human can't do it, and it's will listen to you. Second one, it's like a \"\n",
      " 'human but still have humanoid, but look more stupid that first one. This one '\n",
      " 'only can do something human can do, such as: washing bowls, or mopping '\n",
      " \"floor, etc. The third one didn't look human like human anymore, it's just a \"\n",
      " 'machines, and you know it. This one only can one thing for human, the '\n",
      " \"machines work on the factory it's only can do one thing in one time, other \"\n",
      " \"things will give to another machines to do it. The third one it's not that \"\n",
      " 'useful than first one. Do you see the different between these three '\n",
      " 'robots?\\n')\n",
      "'-             </text>\\n'\n",
      "'-         </writing>\\n'\n",
      "'-     </writings>\\n'\n",
      "'- </selection>\\n'\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "from pprint import pprint\n",
    "test_file = os.path.join(os.pardir, 'Original Files', 'Level 16 EF_camdat.txt')\n",
    "with open(test_file, \"r\") as file:\n",
    "    raw_test = file.read()\n",
    "    clean_test = remove_control_characters(wrap_cdata(raw_test))\n",
    "\n",
    "d = difflib.Differ()\n",
    "text1 = raw_test.splitlines(keepends=True)\n",
    "text2 = clean_test.splitlines(keepends=True)\n",
    "\n",
    "result = list(d.compare(text1,text2))\n",
    "for line in result[-100:]:\n",
    "    if line[0] in ['-','+','?']:\n",
    "        pprint(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09006c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_writings(cleanish_xml):\n",
    "    return etree.fromstring(bytes(cleanish_xml, encoding='utf8'))[1]\n",
    "\n",
    "# parsed_xml_writings = get_writings(controls_removed)\n",
    "# print(\"The number of \\'samples\\' in this level {:,}:\".format(len(parsed_xml_writings)))\n",
    "#some of these samples are obviously useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6d3a8",
   "metadata": {},
   "source": [
    "### This is looking good! Now we need to build a dataframe and extract the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ed209a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just hard code this...\n",
    "\n",
    "def xml_framer(xml_root, cols):\n",
    "    lol = [] # list of lists\n",
    "    for sample in xml_root:\n",
    "        l = []\n",
    "        l.append(sample.attrib['id'])\n",
    "        l.append(sample.attrib['level'])\n",
    "        l.append(sample.attrib['unit'])\n",
    "        l.append(sample[0].attrib['id'])\n",
    "        l.append(sample[0].attrib['nationality'])\n",
    "        l.append(sample[1].text)\n",
    "        l.append(sample[1].attrib['id'])\n",
    "        l.append(sample[2].text)\n",
    "        l.append(sample[3].text)\n",
    "        l.append(sample[4].text)\n",
    "        lol.append(l)\n",
    "    df = pd.DataFrame(lol, columns=cols)\n",
    "    return df\n",
    "\n",
    "col_labels = ['id','lvl','unit','author_id','author_nationality','topic','topic_id','date','grade','text']\n",
    "\n",
    "# test_df = xml_framer(parsed_xml_writings, col_labels)\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a8172",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "If you have the RAM, might as well...\n",
    "\n",
    "**Manually fix the mismatched tags (`<user>Theodora Alexopoulou</url>`) in line 8 of Level 6 EF_camdat.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a7b2493c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lvl</th>\n",
       "      <th>unit</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_nationality</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>date</th>\n",
       "      <th>grade</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C18217</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20967052</td>\n",
       "      <td>cn</td>\n",
       "      <td>Writing labels for a clothing store</td>\n",
       "      <td>22440</td>\n",
       "      <td>2012-04-20 21:12:14.890</td>\n",
       "      <td>79</td>\n",
       "      <td>Date:monday 11th.   Time:9.30 am. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21016113</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-24 03:50:49.100</td>\n",
       "      <td>85</td>\n",
       "      <td>Dear teacher, My name's Yi Zhao,En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C18648</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20967075</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2012-04-20 08:53:55.087</td>\n",
       "      <td>90</td>\n",
       "      <td>My name's Henry Hong,  I was born ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C20184</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Introducing yourself by email</td>\n",
       "      <td>3535</td>\n",
       "      <td>2011-12-14 08:54:48.380</td>\n",
       "      <td>95</td>\n",
       "      <td>Dear Sir, I'm Jianwen Zhang, from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C20185</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18898793</td>\n",
       "      <td>cn</td>\n",
       "      <td>Taking inventory in the office</td>\n",
       "      <td>9820</td>\n",
       "      <td>2011-12-26 09:49:48.140</td>\n",
       "      <td>98</td>\n",
       "      <td>Dear Ms Thomas, There are thirteen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549276</th>\n",
       "      <td>U718945</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>21649452</td>\n",
       "      <td>us</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-04 02:47:23.180</td>\n",
       "      <td>65</td>\n",
       "      <td>THE SPACE NEEDLE The United States...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549277</th>\n",
       "      <td>U719493</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>24750663</td>\n",
       "      <td>ae</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-05 20:41:18.933</td>\n",
       "      <td>0</td>\n",
       "      <td>This is a most new matter of proto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549278</th>\n",
       "      <td>U722572</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>22117882</td>\n",
       "      <td>br</td>\n",
       "      <td>Using creative writing techniques</td>\n",
       "      <td>9146</td>\n",
       "      <td>2012-09-17 20:59:34.413</td>\n",
       "      <td>96</td>\n",
       "      <td>Hi Tabby,   Never mind about askin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549279</th>\n",
       "      <td>U724086</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>19328467</td>\n",
       "      <td>sa</td>\n",
       "      <td>Writing about a symbol of your country</td>\n",
       "      <td>8341</td>\n",
       "      <td>2012-09-23 06:55:59.733</td>\n",
       "      <td>94</td>\n",
       "      <td>Oil Tower Statue   It is a statue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549280</th>\n",
       "      <td>U725092</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>19643771</td>\n",
       "      <td>tw</td>\n",
       "      <td>Attending a robotics conference</td>\n",
       "      <td>7524</td>\n",
       "      <td>2012-09-27 21:40:29.393</td>\n",
       "      <td>85</td>\n",
       "      <td>The human history had thousands ye...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549281 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id lvl unit author_id author_nationality  \\\n",
       "0        C18217   1    7  20967052                 cn   \n",
       "1        C18541   1    1  21016113                 cn   \n",
       "2        C18648   1    1  20967075                 cn   \n",
       "3        C20184   1    1  18898793                 cn   \n",
       "4        C20185   1    2  18898793                 cn   \n",
       "...         ...  ..  ...       ...                ...   \n",
       "549276  U718945  16    2  21649452                 us   \n",
       "549277  U719493  16    1  24750663                 ae   \n",
       "549278  U722572  16    5  22117882                 br   \n",
       "549279  U724086  16    2  19328467                 sa   \n",
       "549280  U725092  16    1  19643771                 tw   \n",
       "\n",
       "                                         topic topic_id  \\\n",
       "0          Writing labels for a clothing store    22440   \n",
       "1                Introducing yourself by email     3535   \n",
       "2                Introducing yourself by email     3535   \n",
       "3                Introducing yourself by email     3535   \n",
       "4               Taking inventory in the office     9820   \n",
       "...                                        ...      ...   \n",
       "549276  Writing about a symbol of your country     8341   \n",
       "549277         Attending a robotics conference     7524   \n",
       "549278       Using creative writing techniques     9146   \n",
       "549279  Writing about a symbol of your country     8341   \n",
       "549280         Attending a robotics conference     7524   \n",
       "\n",
       "                           date grade  \\\n",
       "0       2012-04-20 21:12:14.890    79   \n",
       "1       2011-12-24 03:50:49.100    85   \n",
       "2       2012-04-20 08:53:55.087    90   \n",
       "3       2011-12-14 08:54:48.380    95   \n",
       "4       2011-12-26 09:49:48.140    98   \n",
       "...                         ...   ...   \n",
       "549276  2012-09-04 02:47:23.180    65   \n",
       "549277  2012-09-05 20:41:18.933     0   \n",
       "549278  2012-09-17 20:59:34.413    96   \n",
       "549279  2012-09-23 06:55:59.733    94   \n",
       "549280  2012-09-27 21:40:29.393    85   \n",
       "\n",
       "                                                     text  \n",
       "0                   Date:monday 11th.   Time:9.30 am. ...  \n",
       "1                   Dear teacher, My name's Yi Zhao,En...  \n",
       "2                   My name's Henry Hong,  I was born ...  \n",
       "3                   Dear Sir, I'm Jianwen Zhang, from ...  \n",
       "4                   Dear Ms Thomas, There are thirteen...  \n",
       "...                                                   ...  \n",
       "549276              THE SPACE NEEDLE The United States...  \n",
       "549277              This is a most new matter of proto...  \n",
       "549278              Hi Tabby,   Never mind about askin...  \n",
       "549279              Oil Tower Statue   It is a statue ...  \n",
       "549280              The human history had thousands ye...  \n",
       "\n",
       "[549281 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=col_labels)\n",
    "for i in range(1,17):\n",
    "    file = os.path.join(os.pardir, 'Original Files', ' '.join(['Level', str(i), 'EF_camdat.txt']))\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        df = df.append(xml_framer(get_writings(remove_control_characters(wrap_cdata(data))), col_labels)).reset_index(drop=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e2e328e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3535     42058\n",
       "9820     25742\n",
       "8967     22243\n",
       "5322     21124\n",
       "8572     17395\n",
       "         ...  \n",
       "28243       47\n",
       "23235       45\n",
       "28282       39\n",
       "225         21\n",
       "28257       13\n",
       "Name: topic_id, Length: 156, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(set(df['topic_id'])))\n",
    "\n",
    "display(df['topic_id'].value_counts()[df['topic_id'].value_counts() > 10])\n",
    "display(df['topic_id'].value_counts()[df['topic_id'].value_counts() > 10])\n",
    "# display(df[df['text'].str.contains('</br>', regex=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10b9c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_levels.csv') # about 200MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445dfee",
   "metadata": {},
   "source": [
    "### We may want to consider running a spellchecker on these responses.\n",
    "\n",
    "Maybe we...\n",
    "\n",
    "1. Eliminate useless responses. I think we can assume that all nearly-identical responses have language taken from the prompt. Even if these responses are not just echos of the prompt, I do not think they can tell us very much about the writer, since there is very little variation between them. Like most things, I'm not sure about this.\n",
    "\n",
    "    a. I think it could be fun to calculate levenshtein distance on the responses, and eliminate responses that are too similar that way.\n",
    "    \n",
    "    b. I think the more robust method would be to calculate the TF-IDF and cosine similarity, but that seems a little complex to just find responses with low variance.\n",
    "    \n",
    "    c. We could use a cutoff with the \"Grade attribute\". Irrelevant or incomplete responses are graded lower (maybe anything below 50? below 70?).\n",
    "    \n",
    "2. Once we have a relatively useful subset of samples, we can bifurcate\n",
    "\n",
    "    a. Create .spacy on the samples\n",
    "    \n",
    "    b. Run spellchecker then create .spacy on a copy of the samples. This might not buy us anything, but it just might make a difference.\n",
    "    \n",
    "    **Actually, I think the best way to do this would be through spaCy.** I don't know if spaCy has an in-built spell checker, but we could add a spell checker to a custom spaCy pipeline. This would have the important advantage of preserving the original, misspelled response as well as a reasonable prediction of the intended orthography. I am not sure how we could incorporate both the uncorrected and corrected texts into a single model, but I sort of like the workflow here anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b735677",
   "metadata": {},
   "source": [
    "### Errors I've noticed\n",
    "1. Spaces before commas. I think this is **always** an error.\n",
    "2. No space after commas. I think this is an error unless the character after the comma is a quotation mark.\n",
    "3. Spaces before apostrophes/inverted commas. I think this is **always** an error.\n",
    "4. Misspellings. I suspect we can improve the data automatically by spell checking, but we of course lose information about writers' spelling knowledge.\n",
    "5. No space after periods (at end of sentence). This one is tricky. Periods should have a space when they are separating a sentence, but not when they are separating numbers (69.00 dollars) or acronyms (U.S.A.). \n",
    "\n",
    "spaCy handles most of these errors pretty well I think. It's probably not worth worrying about them. Even where I have marked the error as highly predictable (**always**), a lot of testing would be necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
